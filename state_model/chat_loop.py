#!/usr/bin/env python3
"""
chat_loop.py

A practical chat loop using:
- ProcedureLane JSONL generated by procedure_compiler.py
- run_turn() + ControllerState from pydantic_models.py (your llm_confidence_chat_loop runtime schema)

Responsibilities:
1) Load ProcedureLane(s)
2) Retrieve top-k candidate lanes each turn (deterministic)
3) Maintain ControllerState across turns
4) Call run_turn() to produce assistant response + trace
5) Display comprehensive debugging information

Usage:
  python chat_loop.py --lanes out_nodes.jsonl \
      --model gpt-4o-mini \
      --api-key $OPENAI_API_KEY \
      --debug  # Enable detailed debugging output

Notes:
- Retrieval is intentionally deterministic + cheap (token overlap scoring).
- You can later swap retrieval with vector search; keep the same interface.
- Debug output shows: retrieval priors, belief updates, policy decisions, fact extraction
"""

from __future__ import annotations

import argparse
import asyncio
import json
import os
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from dotenv import load_dotenv

load_dotenv()

# Import your runtime architecture
from llm_confidence_chat_loop import (  # type: ignore
    ControllerState,
    LLMClient,
    LLMConfig,
    ProcedureLane,
    run_turn,
)

from embedding_retrieval import EmbeddingLaneStore

# For pretty printing debug info
try:
    from rich.console import Console
    from rich.table import Table
    from rich.panel import Panel
    from rich.markdown import Markdown
    from rich.tree import Tree
    from rich.syntax import Syntax
    RICH_AVAILABLE = True
    console = Console()
except ImportError:
    RICH_AVAILABLE = False
    console = None
    print("[WARN] 'rich' package not installed. Install with: pip install rich")
    print("[WARN] Falling back to basic text output.\n")


# ----------------------------
# Small deterministic utilities
# ----------------------------

_WORD_RE = re.compile(r"[a-z0-9]+")


def _tokenize(text: str) -> List[str]:
    return _WORD_RE.findall((text or "").lower())


def _jaccard(a: List[str], b: List[str]) -> float:
    sa, sb = set(a), set(b)
    if not sa or not sb:
        return 0.0
    return len(sa & sb) / max(1, len(sa | sb))


def _safe_load_jsonl(path: Path) -> List[Dict[str, Any]]:
    rows: List[Dict[str, Any]] = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            line = (line or "").strip()
            if not line:
                continue
            rows.append(json.loads(line))
    return rows


def _safe_load_json(path: Path) -> Any:
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


# ----------------------------
# Lane sanitation (critical)
# ----------------------------

_NULLABLE_STRING_KEYS = {
    "when",
    "parse_hint",
    "safety_notes",
    "expected_observation",
}


def _sanitize_lane_dict(item: Dict[str, Any]) -> Dict[str, Any]:
    """
    Make lane JSON robust against compiler emitting null for string-ish fields.
    """
    if not isinstance(item, dict):
        return item

    def fix(obj: Any) -> None:
        if isinstance(obj, dict):
            for k, v in list(obj.items()):
                if k in _NULLABLE_STRING_KEYS and v is None:
                    obj[k] = ""
                    continue
                fix(v)
        elif isinstance(obj, list):
            for x in obj:
                fix(x)

    fix(item)

    try:
        steps = item.get("steps")
        if isinstance(steps, list):
            for st in steps:
                if not isinstance(st, dict):
                    continue
                eas = st.get("evidence_actions")
                if isinstance(eas, list):
                    for ea in eas:
                        if not isinstance(ea, dict):
                            continue
                        req = ea.get("request")
                        if isinstance(req, dict) and req.get("parse_hint") is None:
                            req["parse_hint"] = ""
    except Exception:
        pass

    try:
        if (not item.get("entry_step_id")) and isinstance(item.get("steps"), list) and item["steps"]:
            first = item["steps"][0]
            if isinstance(first, dict) and first.get("step_id"):
                item["entry_step_id"] = first["step_id"]
    except Exception:
        pass

    return item


# ----------------------------
# Debug output helpers
# ----------------------------

def print_separator(title: str = "", debug: bool = True):
    """Print a visual separator"""
    if not debug:
        return
    if RICH_AVAILABLE and console:
        console.print(f"\n{'='*80}")
        if title:
            console.print(f"[bold cyan]{title}[/bold cyan]")
        console.print(f"{'='*80}\n")
    else:
        print(f"\n{'='*80}")
        if title:
            print(f">>> {title}")
        print(f"{'='*80}\n")


def print_retrieval_info(query: str, candidates: List[ProcedureLane], priors: Dict[str, float], debug: bool = True):
    """Display retrieval results and priors"""
    if not debug:
        return

    if RICH_AVAILABLE and console:
        table = Table(title="üîç Lane Retrieval", show_header=True, header_style="bold magenta")
        table.add_column("Rank", style="dim", width=6)
        table.add_column("Lane ID", style="cyan")
        table.add_column("Title", style="white")
        table.add_column("Jaccard Score", justify="right", style="green")
        
        for i, lane in enumerate(candidates, 1):
            score = priors.get(lane.lane_id, 0.0)
            table.add_row(
                f"#{i}",
                lane.lane_id,
                lane.title[:50] + "..." if len(lane.title) > 50 else lane.title,
                f"{score:.4f}"
            )
        
        console.print(Panel(table, border_style="blue"))
    else:
        print("\nüîç LANE RETRIEVAL:")
        print(f"Query: {query}\n")
        for i, lane in enumerate(candidates, 1):
            score = priors.get(lane.lane_id, 0.0)
            print(f"  #{i} [{score:.4f}] {lane.lane_id}: {lane.title}")


def print_extraction_info(extraction: Dict[str, Any], debug: bool = True):
    """Display extracted facts and evidence"""
    if not debug:
        return

    if RICH_AVAILABLE and console:
        # Facts table
        if extraction.get("extracted_facts"):
            table = Table(title="üìã Extracted Facts", show_header=True, header_style="bold yellow")
            table.add_column("Fact Key", style="cyan")
            table.add_column("Value", style="white")
            table.add_column("Confidence", justify="right", style="green")
            table.add_column("Source", style="dim")
            
            for fact in extraction["extracted_facts"]:
                table.add_row(
                    fact["fact"],
                    str(fact.get("value", ""))[:40],
                    f"{fact.get('confidence', 0):.2f}",
                    fact.get("source", "unknown")
                )
            console.print(table)
        
        # Notes
        if extraction.get("notes"):
            console.print("\n[bold]Notes:[/bold]", style="yellow")
            for note in extraction["notes"]:
                console.print(f"  ‚Ä¢ {note}", style="dim")
    else:
        print("\nüìã EXTRACTED FACTS:")
        for fact in extraction.get("extracted_facts", []):
            print(f"  ‚Ä¢ {fact['fact']} = {fact.get('value')} (conf: {fact.get('confidence', 0):.2f}, source: {fact.get('source')})")
        
        if extraction.get("notes"):
            print("\nNotes:")
            for note in extraction["notes"]:
                print(f"  ‚Ä¢ {note}")


def print_belief_update(belief_trace: Dict[str, Any], debug: bool = True):
    """Display belief state changes"""
    if not debug:
        return

    if RICH_AVAILABLE and console:
        # Before/After comparison
        table = Table(title="üéØ Belief Update", show_header=True, header_style="bold green")
        table.add_column("Lane", style="cyan")
        table.add_column("Before", justify="right", style="yellow")
        table.add_column("After", justify="right", style="green")
        table.add_column("Change", justify="right", style="magenta")
        table.add_column("Reason", style="dim", width=30)
        
        # Build lookup for deltas
        delta_map = {}
        for d in belief_trace.get("deltas", []):
            delta_map[d["key"]] = d
        
        # Show top lanes
        for item in belief_trace.get("topk_after", [])[:6]:
            lane_id = item["key"]
            after = item["belief"]
            
            # Find before value
            before = 0.0
            for b_item in belief_trace.get("topk_before", []):
                if b_item["key"] == lane_id:
                    before = b_item["belief"]
                    break
            
            change = after - before
            reason = delta_map.get(lane_id, {}).get("reason", "-")
            
            change_str = f"{change:+.3f}" if abs(change) > 0.001 else "¬±0.000"
            table.add_row(
                lane_id,
                f"{before:.3f}",
                f"{after:.3f}",
                change_str,
                reason[:30]
            )
        
        console.print(table)
        
        # Threshold info
        threshold = belief_trace.get("threshold_value", 0.6)
        crossed = belief_trace.get("threshold_crossed", False)
        phase = belief_trace.get("phase", "unknown")
        
        info = f"Phase: [bold]{phase}[/bold] | Threshold: {threshold:.2f} | Crossed: {'‚úÖ' if crossed else '‚ùå'}"
        console.print(Panel(info, border_style="cyan", title="State"))
        
    else:
        print("\nüéØ BELIEF UPDATE:")
        print(f"Phase: {belief_trace.get('phase')}")
        print(f"Threshold: {belief_trace.get('threshold_value', 0):.2f} (Crossed: {belief_trace.get('threshold_crossed', False)})")
        print("\nTop lanes (before -> after):")
        
        delta_map = {d["key"]: d for d in belief_trace.get("deltas", [])}
        
        for item in belief_trace.get("topk_after", [])[:6]:
            lane_id = item["key"]
            after = item["belief"]
            
            before = 0.0
            for b_item in belief_trace.get("topk_before", []):
                if b_item["key"] == lane_id:
                    before = b_item["belief"]
                    break
            
            change = after - before
            reason = delta_map.get(lane_id, {}).get("reason", "-")
            
            print(f"  {lane_id}: {before:.3f} -> {after:.3f} ({change:+.3f}) | {reason}")


def print_policy_decision(policy: Dict[str, Any], debug: bool = True):
    """Display policy decision and reasoning"""
    if not debug:
        return

    action = policy.get("selected_action", "UNKNOWN")
    reason_code = policy.get("reason_code", "")
    rationale = policy.get("rationale", "")
    
    if RICH_AVAILABLE and console:
        content = f"""
**Action:** {action}
**Reason Code:** {reason_code}
**Rationale:** {rationale}
        """
        
        if policy.get("selected_lane_id"):
            content += f"\n**Lane:** {policy['selected_lane_id']}"
        if policy.get("selected_step_id"):
            content += f"\n**Step:** {policy['selected_step_id']}"
        if policy.get("selected_evidence_action_id"):
            content += f"\n**Evidence Action:** {policy['selected_evidence_action_id']}"
        
        console.print(Panel(
            Markdown(content),
            title="ü§ñ Policy Decision",
            border_style="yellow"
        ))
    else:
        print("\nü§ñ POLICY DECISION:")
        print(f"  Action: {action}")
        print(f"  Reason Code: {reason_code}")
        print(f"  Rationale: {rationale}")
        if policy.get("selected_lane_id"):
            print(f"  Lane: {policy['selected_lane_id']}")
        if policy.get("selected_step_id"):
            print(f"  Step: {policy['selected_step_id']}")
        if policy.get("selected_evidence_action_id"):
            print(f"  Evidence Action: {policy['selected_evidence_action_id']}")


def print_step_execution(step_trace: Optional[Dict[str, Any]], debug: bool = True):
    """Display step execution details"""
    if not debug or not step_trace:
        return

    if RICH_AVAILABLE and console:
        tree = Tree("üìù Step Execution")
        tree.add(f"Lane: [cyan]{step_trace.get('lane_id')}[/cyan]")
        tree.add(f"Step: [cyan]{step_trace.get('step_id')}[/cyan]")
        tree.add(f"Action: [yellow]{step_trace.get('action_taken')}[/yellow]")
        
        if step_trace.get("validation_results"):
            val_node = tree.add("Validations:")
            for rule_id, passed in step_trace["validation_results"]:
                status = "‚úÖ" if passed else "‚ùå"
                val_node.add(f"{status} {rule_id}")
        
        if step_trace.get("transition_taken"):
            tree.add(f"Transition: [green]{step_trace['transition_taken']}[/green]")
        
        if step_trace.get("notes"):
            notes_node = tree.add("Notes:")
            for note in step_trace["notes"]:
                notes_node.add(note)
        
        console.print(tree)
    else:
        print("\nüìù STEP EXECUTION:")
        print(f"  Lane: {step_trace.get('lane_id')}")
        print(f"  Step: {step_trace.get('step_id')}")
        print(f"  Action: {step_trace.get('action_taken')}")
        
        if step_trace.get("validation_results"):
            print("  Validations:")
            for rule_id, passed in step_trace["validation_results"]:
                status = "‚úÖ" if passed else "‚ùå"
                print(f"    {status} {rule_id}")
        
        if step_trace.get("transition_taken"):
            print(f"  Transition: {step_trace['transition_taken']}")


def print_counters(counters: Dict[str, Any], budgets: Dict[str, Any], debug: bool = True):
    """Display runtime counters and budgets"""
    if not debug:
        return

    if RICH_AVAILABLE and console:
        table = Table(title="üìä Counters & Budgets", show_header=True, header_style="bold blue")
        table.add_column("Metric", style="cyan")
        table.add_column("Current", justify="right", style="yellow")
        table.add_column("Budget", justify="right", style="green")
        table.add_column("Status", justify="center")
        
        metrics = [
            ("Total Turns", counters.get("total_turns", 0), budgets.get("max_total_turns", 20)),
            ("Total Questions", counters.get("total_questions", 0), budgets.get("max_total_questions", 8)),
            ("Precommit Questions", counters.get("precommit_questions", 0), budgets.get("max_precommit_questions", 3)),
            ("Failed Steps", counters.get("failed_steps", 0), budgets.get("max_failed_steps_before_escalate", 2)),
            ("Steps in Lane", counters.get("steps_executed_in_lane", 0), budgets.get("max_steps_before_reconsider", 3)),
        ]
        
        for name, current, budget in metrics:
            if current >= budget:
                status = "üî¥"
            elif current >= budget * 0.7:
                status = "üü°"
            else:
                status = "üü¢"
            
            table.add_row(name, str(current), str(budget), status)
        
        console.print(table)
    else:
        print("\nüìä COUNTERS & BUDGETS:")
        print(f"  Total Turns: {counters.get('total_turns', 0)} / {budgets.get('max_total_turns', 20)}")
        print(f"  Total Questions: {counters.get('total_questions', 0)} / {budgets.get('max_total_questions', 8)}")
        print(f"  Precommit Questions: {counters.get('precommit_questions', 0)} / {budgets.get('max_precommit_questions', 3)}")
        print(f"  Failed Steps: {counters.get('failed_steps', 0)} / {budgets.get('max_failed_steps_before_escalate', 2)}")
        print(f"  Steps in Lane: {counters.get('steps_executed_in_lane', 0)} / {budgets.get('max_steps_before_reconsider', 3)}")


def print_warnings(warnings: List[str], debug: bool = True):
    """Display any warnings from the turn"""
    if not debug or not warnings:
        return

    if RICH_AVAILABLE and console:
        console.print("\n[bold red]‚ö†Ô∏è  Warnings:[/bold red]")
        for w in warnings:
            console.print(f"  ‚Ä¢ {w}", style="red")
    else:
        print("\n‚ö†Ô∏è  WARNINGS:")
        for w in warnings:
            print(f"  ‚Ä¢ {w}")


def print_turn_summary(turn_num: int, response_text: str, debug: bool = True):
    """Print the final assistant response"""
    if RICH_AVAILABLE and console:
        console.print(Panel(
            response_text,
            title=f"üí¨ Assistant Response (Turn {turn_num})",
            border_style="green",
            padding=(1, 2)
        ))
    else:
        print(f"\nüí¨ ASSISTANT RESPONSE (Turn {turn_num}):")
        print(f"{response_text}\n")


# ----------------------------
# Lane store + retrieval
# ----------------------------

@dataclass
class RetrievalConfig:
    top_k: int = 5
    min_score: float = 0.02
    prefer_current_lane: bool = True


class LaneStore:
    def __init__(self, lanes: List[ProcedureLane]):
        self.lanes = lanes
        self._index = self._build_index(lanes)

    @staticmethod
    def load(path: str) -> "LaneStore":
        p = Path(path)
        if not p.exists():
            raise FileNotFoundError(f"Lane file not found: {p}")

        lanes_raw: List[Dict[str, Any]]
        if p.suffix.lower() in (".jsonl", ".jl"):
            lanes_raw = _safe_load_jsonl(p)
        else:
            obj = _safe_load_json(p)
            if isinstance(obj, list):
                lanes_raw = obj
            elif isinstance(obj, dict) and "lanes" in obj and isinstance(obj["lanes"], list):
                lanes_raw = obj["lanes"]
            else:
                raise ValueError("Unsupported lane file format. Use JSONL or JSON array.")

        lanes: List[ProcedureLane] = []
        errors: List[str] = []

        for i, item in enumerate(lanes_raw):
            if not isinstance(item, dict):
                errors.append(f"Row {i}: not a JSON object; skipping.")
                continue

            item = _sanitize_lane_dict(item)

            try:
                lanes.append(ProcedureLane.model_validate(item))
            except Exception as e:
                pid = item.get("procedure_id") or item.get("lane_id") or item.get("doc_id") or f"row:{i}"
                errors.append(f"Failed to parse lane {pid}: {e}")

        if not lanes:
            joined = "\n".join(errors[:20])
            raise ValueError(f"No lanes could be loaded. Sample errors:\n{joined}")

        if errors:
            print("\n[WARN] Some lanes failed to load (showing up to 10):")
            for msg in errors[:10]:
                print(" -", msg)
            print()

        return LaneStore(lanes)

    @staticmethod
    def _build_index(lanes: List[ProcedureLane]) -> List[Tuple[str, List[str]]]:
        idx: List[Tuple[str, List[str]]] = []
        for lane in lanes:
            blob = " ".join(
                [
                    lane.lane_id,
                    lane.doc_id,
                    lane.doc_title,
                    lane.title,
                    lane.summary,
                    " ".join([fs.fact for fs in (lane.fact_specs or [])]),
                    " ".join([s.title or "" for s in (lane.steps[:3] if lane.steps else [])]),
                    " ".join([s.instruction[:160] for s in (lane.steps[:2] if lane.steps else [])]),
                    " ".join([t for t in (lane.tags or [])[:20]]),
                    " ".join([sig for sig in (lane.signatures or [])[:20]]),
                ]
            )
            idx.append((lane.lane_id, _tokenize(blob)))
        return idx

    def get_lane(self, lane_id: str) -> Optional[ProcedureLane]:
        for l in self.lanes:
            if l.lane_id == lane_id:
                return l
        return None

    def retrieve(
        self,
        query_text: str,
        cfg: RetrievalConfig,
        *,
        controller_state: Optional[ControllerState] = None,
    ) -> Tuple[List[ProcedureLane], Dict[str, float]]:
        """
        Returns (lanes, priors_dict) where priors_dict maps lane_id -> jaccard_score
        
        CRITICAL: Maintains stability by always including lanes with existing belief
        """
        q = _tokenize(query_text)
        scored: List[Tuple[float, ProcedureLane]] = []
        priors: Dict[str, float] = {}
        
        current_lane_id = controller_state.current_lane_id if controller_state else None
        
        # Get lanes that currently have belief > 0
        lanes_with_belief: Dict[str, float] = {}
        if controller_state and controller_state.belief.items:
            for item in controller_state.belief.items:
                if item.belief > 0.01:  # Threshold to ignore noise
                    lanes_with_belief[item.key] = item.belief
        
        lanes_to_keep: Dict[str, ProcedureLane] = {}

        for (lane_id, tokens) in self._index:
            lane = self.get_lane(lane_id)
            if lane is None:
                continue

            score = _jaccard(q, tokens)
            
            # CRITICAL: Strong boost for lanes that already have belief
            if lane_id in lanes_with_belief:
                # Boost proportional to existing belief to maintain stability
                boost_factor = 1.5 + (lanes_with_belief[lane_id] * 2.0)  # 1.5x to 3.5x
                score *= boost_factor
                lanes_to_keep[lane_id] = lane
            
            # Extra boost for current lane
            if cfg.prefer_current_lane and current_lane_id == lane_id:
                score *= 2.0
                lanes_to_keep[lane_id] = lane

            priors[lane_id] = score

            if score >= cfg.min_score:
                scored.append((score, lane))

        scored.sort(key=lambda x: x[0], reverse=True)
        
        # Build result ensuring we keep lanes with existing belief
        result_lanes = [l for _, l in scored[: cfg.top_k]]
        
        # CRITICAL: Force inclusion of lanes that have non-zero belief
        for lane_id, lane in lanes_to_keep.items():
            if lane not in result_lanes:
                result_lanes.insert(0, lane)
        
        # Trim to top_k (but prioritize lanes with belief)
        result_lanes = result_lanes[:cfg.top_k]

        if not result_lanes:
            fallback_lanes = self.lanes[: max(1, cfg.top_k)]
            return fallback_lanes, priors

        return result_lanes, priors



# ----------------------------
# Session wrapper
# ----------------------------

class ChatSession:
    """
    Holds controller state + runs turns.
    """

    def __init__(
        self,
        *,
        lane_store: LaneStore,
        llm_extractor: LLMClient,
        llm_controller: LLMClient,
        retrieval_cfg: RetrievalConfig,
        session_ctx: Optional[Dict[str, Any]] = None,
        debug: bool = False,
    ):
        self.lane_store = lane_store
        self.llm_extractor = llm_extractor
        self.llm_controller = llm_controller
        self.retrieval_cfg = retrieval_cfg
        self.ctx = session_ctx or {}
        self.debug = debug

        self.state = ControllerState()
        self.turn_traces: List[Dict[str, Any]] = []
        
        # Track conversation history for context
        self.conversation_history: List[Tuple[str, str]] = []  # [(user, assistant), ...]

    async def handle_user_turn(
        self,
        user_text: str,
        *,
        artifacts: Optional[List[Dict[str, Any]]] = None,
        slots: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        One user message -> assistant response text
        """
        turn_num = self.state.counters.total_turns + 1
        
        if self.debug:
            print_separator(f"TURN {turn_num}: Processing User Input", debug=self.debug)
            if RICH_AVAILABLE and console:
                console.print(Panel(user_text, title="üë§ User Input", border_style="blue"))
            else:
                print(f"üë§ User: {user_text}\n")

        # Retrieve candidates
        candidates, priors = self.lane_store.retrieve(
            user_text,
            self.retrieval_cfg,
            controller_state=self.state,
        )

        # Add priors to context for deterministic belief updates
        self.ctx["retrieval_priors"] = priors
        
        # Build conversation history for extractor context (last 3 turns)
        history_text = ""
        if self.conversation_history:
            recent = self.conversation_history[-3:]  # Last 3 turns
            history_parts = []
            for user_msg, assistant_msg in recent:
                history_parts.append(f"User: {user_msg}")
                history_parts.append(f"Assistant: {assistant_msg}")
            history_text = "\n".join(history_parts)
        
        self.ctx["history"] = history_text

        if self.debug:
            print_retrieval_info(user_text, candidates, priors, debug=self.debug)
            if history_text and RICH_AVAILABLE and console:
                console.print(Panel(history_text, title="üìú Conversation Context", border_style="dim"))

        # Run the turn
        result = await run_turn(
            llm_extractor=self.llm_extractor,
            llm_controller=self.llm_controller,
            controller_state=self.state,
            candidate_lanes=candidates,
            user_text=user_text,
            artifacts=artifacts or [],
            slots=slots or {},
            ctx=self.ctx,
        )

        # Display debug info
        if self.debug:
            trace = result.trace.model_dump()
            
            print_separator("Extraction Results", debug=self.debug)
            print_extraction_info(trace.get("extraction", {}), debug=self.debug)
            
            print_separator("Belief Update", debug=self.debug)
            print_belief_update(trace.get("belief", {}), debug=self.debug)
            
            print_separator("Policy Decision", debug=self.debug)
            print_policy_decision(trace.get("policy", {}), debug=self.debug)
            
            if trace.get("step"):
                print_separator("Step Execution", debug=self.debug)
                print_step_execution(trace.get("step"), debug=self.debug)
            
            print_separator("Runtime Status", debug=self.debug)
            print_counters(
                result.state.counters.model_dump(),
                result.state.budgets.model_dump(),
                debug=self.debug
            )
            
            if trace.get("warnings"):
                print_warnings(trace["warnings"], debug=self.debug)
            
            print_separator("", debug=self.debug)

        # Persist updated state + trace
        self.state = result.state
        self.turn_traces.append(result.trace.model_dump())
        
        # Store in conversation history
        self.conversation_history.append((user_text, result.response_text))

        # Print response
        if self.debug:
            print_turn_summary(turn_num, result.response_text, debug=self.debug)
        
        return result.response_text

    def export_traces(self, output_path: str) -> None:
        """Export all turn traces to JSON for analysis"""
        with open(output_path, "w") as f:
            json.dump(self.turn_traces, f, indent=2)
        print(f"\n‚úÖ Exported {len(self.turn_traces)} turn traces to {output_path}")


# ----------------------------
# CLI demo loop
# ----------------------------

async def interactive_cli(session: ChatSession, debug: bool = False) -> None:
    if RICH_AVAILABLE and console:
        console.print(Panel(
            "[bold green]Welcome to the IT Support Agent[/bold green]\n\n"
            "Type your issue to get help. Commands:\n"
            "  ‚Ä¢ [cyan]exit[/cyan] or [cyan]quit[/cyan] - End the session\n"
            "  ‚Ä¢ [cyan]export <filename>[/cyan] - Export debug traces\n"
            "  ‚Ä¢ [cyan]state[/cyan] - Show current controller state\n",
            title="ü§ñ Agent Chat Loop",
            border_style="cyan"
        ))
    else:
        print("\n" + "="*80)
        print("Welcome to the IT Support Agent")
        print("="*80)
        print("\nType your issue. Commands: exit, quit, export <filename>, state\n")

    while True:
        try:
            if RICH_AVAILABLE and console:
                user_text = console.input("[bold blue]you>[/bold blue] ").strip()
            else:
                user_text = input("you> ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\n\nGoodbye!")
            break

        if not user_text:
            continue

        if user_text.lower() in ("exit", "quit"):
            print("\nGoodbye!")
            break

        if user_text.lower().startswith("export "):
            filename = user_text.split(None, 1)[1] if len(user_text.split()) > 1 else "traces.json"
            session.export_traces(filename)
            continue

        if user_text.lower() == "state":
            if RICH_AVAILABLE and console:
                syntax = Syntax(json.dumps(session.state.model_dump(), indent=2), "json", theme="monokai")
                console.print(Panel(syntax, title="Controller State", border_style="cyan"))
            else:
                print("\nCurrent State:")
                print(json.dumps(session.state.model_dump(), indent=2))
            continue

        artifacts: List[Dict[str, Any]] = []
        resp = await session.handle_user_turn(user_text, artifacts=artifacts)
        
        if not debug:
            # Only print response if not in debug mode (debug mode already prints it)
            if RICH_AVAILABLE and console:
                console.print(f"\n[bold green]assistant>[/bold green] {resp}\n")
            else:
                print(f"\nassistant> {resp}\n")


def main() -> None:
    ap = argparse.ArgumentParser(description="IT Support Agent Chat Loop with Debug Mode")
    ap.add_argument("--lanes", required=True, help="Path to ProcedureLane JSONL/JSON")
    ap.add_argument("--model", default=os.getenv("OPENAI_MODEL", "gpt-4o-mini"))
    ap.add_argument("--api-key", default=os.getenv("OPENAI_API_KEY"))
    ap.add_argument("--base-url", default=os.getenv("OPENAI_BASE_URL"))
    ap.add_argument("--top-k", type=int, default=int(os.getenv("RETRIEVE_TOPK", "5")))
    ap.add_argument("--min-score", type=float, default=float(os.getenv("RETRIEVE_MIN_SCORE", "0.02")))
    ap.add_argument("--debug", action="store_true", help="Enable detailed debug output")
    ap.add_argument("--export-on-exit", help="Auto-export traces to this file on exit")
    
    # Embedding options
    ap.add_argument("--use-embeddings", action="store_true", 
                    help="Use semantic embeddings instead of keyword matching (RECOMMENDED for better accuracy)")
    ap.add_argument("--embedding-model", 
                    default=os.getenv("EMBEDDING_MODEL", "all-mpnet-base-v2"),
                    help="Embedding model: all-mpnet-base-v2 (default, best free), text-embedding-3-large (best quality, requires API)")
    ap.add_argument("--use-openai-embeddings", action="store_true",
                    help="Use OpenAI embeddings (requires --api-key)")
    
    args = ap.parse_args()

    if not args.api_key:
        raise SystemExit("Missing --api-key or OPENAI_API_KEY")

    # Load lane store with optional embedding support
    if args.use_embeddings:
        try:            
            if RICH_AVAILABLE and console:
                console.print(f"\n[cyan]üîÆ Using semantic embeddings[/cyan]")
                console.print(f"   Model: [bold]{args.embedding_model}[/bold]")
                if args.use_openai_embeddings:
                    console.print(f"   Provider: [yellow]OpenAI API[/yellow]")
                else:
                    console.print(f"   Provider: [green]Local (Sentence Transformers)[/green]")
            else:
                print(f"\nüîÆ Using semantic embeddings")
                print(f"   Model: {args.embedding_model}")
            
            lane_store = EmbeddingLaneStore.load(
                lanes_path=args.lanes,
                embedding_model=args.embedding_model,
                use_openai=args.use_openai_embeddings,
                api_key=args.api_key if args.use_openai_embeddings else None,
            )
        except ImportError as e:
            print(f"\n‚ùå Failed to load embedding module: {e}")
            print("Install dependencies:")
            if args.use_openai_embeddings:
                print("  pip install openai numpy")
            else:
                print("  pip install sentence-transformers")
            raise SystemExit(1)
    else:
        if RICH_AVAILABLE and console:
            console.print(f"\n[dim]Using keyword-based retrieval (use --use-embeddings for better accuracy)[/dim]")
        
        lane_store = LaneStore.load(args.lanes)
    
    if RICH_AVAILABLE and console:
        console.print(f"‚úÖ Loaded [bold cyan]{len(lane_store.lanes)}[/bold cyan] procedure lanes from [cyan]{args.lanes}[/cyan]")
    else:
        print(f"\n‚úÖ Loaded {len(lane_store.lanes)} procedure lanes from {args.lanes}")

    llm_cfg = LLMConfig(
        model=args.model,
        api_key=args.api_key,
        base_url=args.base_url,
        timeout_s=30.0,
        max_retries=3,
    )
    llm_extractor = LLMClient(llm_cfg)
    llm_controller = LLMClient(llm_cfg)

    session = ChatSession(
        lane_store=lane_store,
        llm_extractor=llm_extractor,
        llm_controller=llm_controller,
        retrieval_cfg=RetrievalConfig(top_k=args.top_k, min_score=args.min_score),
        session_ctx={"channel": "cli"},
        debug=args.debug,
    )

    try:
        asyncio.run(interactive_cli(session, debug=args.debug))
    finally:
        if args.export_on_exit:
            session.export_traces(args.export_on_exit)


if __name__ == "__main__":
    main()